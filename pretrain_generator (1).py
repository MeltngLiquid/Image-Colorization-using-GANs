# -*- coding: utf-8 -*-
"""pretrain_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S_x2gE3-5IwjEV7rPI32BV_9jgsvXclN
"""

def generate_real_samples(dataset, n_samples, patch_shape):
    # Get a batch from the dataset
    for [trainA, trainB] in dataset:
        # Choose a random instance from the batch
        ix = np.random.randint(0, trainA.shape[0], n_samples)

        # Convert indices to TensorFlow tensor
        ix_tensor = tf.convert_to_tensor(ix, dtype=tf.int32)

        # Select the random instances
        X = tf.gather(trainA, ix_tensor, axis=0)
        Y = tf.gather(trainB, ix_tensor, axis=0)

        # Create real labels (1s for real images)
        y = np.ones((n_samples, patch_shape, patch_shape, 1), dtype=np.float32)


        return [X, Y], y

# generate a batch of images, returns images and targets
def generate_fake_samples(g_model, samples, patch_shape):
    # Generate fake images using the generator model
    X = g_model.predict(samples)
    # Create 'fake' class labels (0)
    y = np.zeros((len(X), patch_shape, patch_shape, 1))  # Using np.zeros, not zeros
    return X, y

def define_gan(g_model, d_model, image_shape):
	# make weights in the discriminator not trainable
	for layer in d_model.layers:
		if not isinstance(layer, BatchNormalization):
			layer.trainable = False       #Descriminator layers set to untrainable in the combined GAN but
                                                #standalone descriminator will be trainable.
	# define the source image
	in_src = Input(shape=image_shape)
	# suppy the image as input to the generator
	gen_out = g_model(in_src)
	# supply the input image and generated image as inputs to the discriminator

	dis_out = d_model([in_src, gen_out])
	# src image as input, generated image and disc. output as outputs
	model = Model(in_src, [dis_out, gen_out])
	# compile model
	opt = Adam(learning_rate=0.0002, beta_1=0.5)

    #Total loss is the weighted sum of adversarial loss (BCE) and L1 loss (MAE)
    #Authors suggested weighting BCE vs L1 as 1:100.
	model.compile(loss=['binary_crossentropy', 'mae'],
               optimizer=opt, loss_weights=[1,100])
	return model

def pretrain_generator(g_model, dataset, n_epochs=1):
    # Compile generator with L1 loss
    g_model.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='mae')

    # Loop over epochs
    for epoch in range(n_epochs):
        print(f"Starting epoch {epoch + 1}/{n_epochs}...")
        batch_idx = 1

        # Iterate over dataset batches
        for X_realA, X_realB in dataset:
            # Update the generator on the current batch
            g_loss = g_model.train_on_batch(X_realA, X_realB)
            print(f"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {g_loss:.3f}")
            batch_idx += 1

    # Save pretrained generator
    g_model.save('pretrained_generator.keras')
    print("Pretrained generator saved.")

image_shape = (256, 256,1 )

# Define generator
g_model = define_generator(image_shape)

# Pretrain the generator
pretrain_generator(g_model, train_dataset)